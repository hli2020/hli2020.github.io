---
layout: post
comments: true
title:  "Minimum Knowledge of CV and ML"
excerpt: "There do exists the minimum knowledge of computer vision and machine learning in order for us to have a basic, fluent, non-embarassing conversation with professionals. There's a broad span of topics in these two fields and we need a bunch of books to cover them all. This blog just walks through the very basic pipeline of the most famous algorithms."
date:   2016-04-01 12:00:00
mathjax: true
---

This term I have finished all my five PhD courses. Most of them cover the basic knowledge in computer vision, machine learning and big data. In this blog I will list the most common, popular and fresh algorithms which people often mention. For the record, I don't know the details of most topics and this on-going project aims at providing the 101 facts in each topic so that you will feel involved in conservations with professionals out of your research focus.


**Note:** This blog is on-going and takes like forever to finish. I have opened the [discussion board](#discuss) at the end of this page and welcome suggestions on contents, bug fixes, etc. :) 

**Plan:**

- Phase 1 (end of April): first half of module 1 and 2. (version **Elena**)<br>
<font color="DarkGrey">
	- Phase 2 (start of June): second half of module 1. (version **Caroline**)<br>
	- Phase 3 (end of June): second half of module 2. (version **Bonny**)<br>
	- Phase 4 (end of July): initial release of the blog. (version **Stephen**)<br>
	- Phase 5 (forever): minor fixes or major additions. (version **Damon**)<br>
</font>


<a name='top'></a>
**Table of Contents:**

Module 1: Computer Vision

- [Introduction](#intro)
- [Low-level Vision and Image Processing](#low)
- [Computational Photography](#graphics)
- [Face and Gesture](#face)
- [Motion Estimation and Tracking](#motion)
- [Detection and Segmentation](#det)
- [Scene Understanding](#scene)
- [Image Captioning and NLP](#caption)
- [3D Computer Vision](#3d)
- [Video Analysis and Action Recognition](#video)
- [Dataset and Evaluation](#eval)

Module 2: Machine Learning

- [Introduction](#intro_ml)
- [Optimization](#opt)
- [Overview of Learning Paradigms](#learning)
- [Ensembel Models]()
- [Generative Models]()
- [Discriminative Methods]()
- [Graphical Model and Exponential Families](#grachical)
- [Variational Inference](#vi)
- [Monte Carlo Sampling](#sample)
- [SVM and K-means on Large-scale Data](#svm)

<a name='intro'></a>

### Introduction  
<!--
**Motivation**. In this section we will develop expertise with an intuitive understanding of **backpropagation**, which is a way of computing gradients of expressions through recursive application of **chain rule**. Understanding of this process and its subtleties is critical for you to understand, and effectively develop, design and debug Neural Networks.

**Problem statement**. The core problem studied in this section is as follows: We are given some function \\(f(x)\\) where \\(x\\) is a vector of inputs and we are interested in computing the gradient of \\(f\\) at \\(x\\) (i.e. \\(\nabla f(x)\\) ).

**Motivation**. Recall that the primary reason we are interested in this problem is that in the specific case of Neural Networks, \\(f\\) will correspond to the loss function ( \\(L\\) ) and the inputs \\(x\\) will consist of the training data and the neural network weights. For example, the loss could be the SVM loss function and the inputs are both the training data \\((x\_i,y\_i), i=1 \ldots N\\) and the weights and biases \\(W,b\\). Note that (as is usually the case in Machine Learning) we think of the training data as given and fixed, and of the weights as variables we have control over. Hence, even though we can easily use backpropagation to compute the gradient on the input examples \\(x\_i\\), in practice we usually only compute the gradient for the parameters (e.g. \\(W,b\\)) so that we can use it to perform a parameter update. However, as we will see later in the class the gradient on \\(x\_i\\) can still be useful sometimes, for example for purposes of visualization and interpreting what the Neural Network might be doing.

If you are coming to this class and you're comfortable with deriving gradients with chain rule, we would still like to encourage you to at least skim this section, since it presents a rarely developed view of backpropagation as backward flow in real-valued circuits and any insights you'll gain may help you throughout the class.
-->

CV Family Tree, Big Guys and Their Groups. I have created (or will) a [page](http://www.ee.cuhk.edu.hk/~yangli/community/) gossipping around.

The following lists are some popular vision and deep learning courses in the wild, with some textbooks that are worth marking. They are subject to change as time evolves and you can always find up-to-date courses and tutorials online. (still thinking whether to put CUHK resources here. roughly below average.)

Popular Courses:

- [cs231n: Convolutional Neural Networks for Visual Recognition.](http://cs231n.stanford.edu/) Featuring concrete, fresh, interesting case studies. Highly recommended and even better than most vision courses offered by professors.
- [cs143: Introduction to Computer Vision.](http://cs.brown.edu/courses/csci1430/2011/) Classical computer vision track course offered by James Hays when he was at Brown University.

- [Introduction to Computer Vision.](http://www.cs.cornell.edu/courses/cs4670/2013fa/) By Noah Snavely at  Cornell.
- [6.869: Advances in Computer Vision.](http://6.869.csail.mit.edu/fa13/) MIT Graduate-level course offered by Antonio Torralba.


- [cs229: Machine Learning.](http://cs229.stanford.edu/) The famous Andrew Ng style course with easy start and good intuitions. There are so many related materials about this course where you can take a peek online.
- [Machine Learning.](http://www.cs.columbia.edu/~jebara/4771/index.html) By Tony Jebara at Comlumbia. My friend at Comlumbia recommended this course and I found the contents are well-covered and solid.


Textbooks:

- [Deep Learning.](http://www.deeplearningbook.org/) By Ian Goodfellow and it covers most necessities.
- [Computer Vision: Algorithms and Applications.](http://szeliski.org/Book/) Most popular among the US. I even bought a hardcopy (cost me too much!) and they have a free e-book online.

- [Pattern Recognition and Machine Learning.](http://research.microsoft.com/en-us/um/people/cmbishop/prml/index.htm) Bible in Machine Learning. Everyone should know at least a tiny bit of it. Don't tell others I said you can illegally obtain the electronic verision online.
- [An Introduction to Statistical Learning.](http://www-bcf.usc.edu/~gareth/ISL/) Maybe a good choice.


<span style="float: right">
	[Table of Contents](#top)
</span>


<a name='low'></a>

### Low-level Vision and Image Processing  


<!-- Lets start simple so that we can develop the notation and conventions for more complex expressions. Consider a simple multiplication function of two numbers \\(f(x,y) = x y\\). It is a matter of simple calculus to derive the partial derivative for either input:

$$
f(x,y) = x y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = y \hspace{0.5in} \frac{\partial f}{\partial y} = x 
$$

**Interpretation**. Keep in mind what the derivatives tell you: They indicate the rate of change of a function with respect to that variable surrounding an infinitesimally small region near a particular point:

$$
\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}
$$

A technical note is that the division sign on the left-hand sign is, unlike the division sign on the right-hand sign, not a division. Instead, this notation indicates that the operator \\(  \frac{d}{dx} \\) is being applied to the function \\(f\\), and returns a different function (the derivative). A nice way to think about the expression above is that when \\(h\\) is very small, then the function is well-approximated by a straight line, and the derivative is its slope. In other words, the derivative on each variable tells you the sensitivity of the whole expression on its value. For example, if \\(x = 4, y = -3\\) then \\(f(x,y) = -12\\) and the derivative on \\(x\\) \\(\frac{\partial f}{\partial x} = -3\\). This tells us that if we were to increase the value of this variable by a tiny amount, the effect on the whole expression would be to decrease it (due to the negative sign), and by three times that amount. This can be seen by rearranging the above equation ( \\( f(x + h) = f(x) + h \frac{df(x)}{dx} \\) ). Analogously, since \\(\frac{\partial f}{\partial y} = 4\\), we expect that increasing the value of \\(y\\) by some very small amount \\(h\\) would also increase the output of the function (due to the positive sign), and by \\(4h\\).

> The derivative on each variable tells you the sensitivity of the whole expression on its value.

As mentioned, the gradient \\(\nabla f\\) is the vector of partial derivatives, so we have that \\(\nabla f = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}] = [y, x]\\). Even though the gradient is technically a vector, we will often use terms such as *"the gradient on x"* instead of the technically correct phrase *"the partial derivative on x"* for simplicity.

We can also derive the derivatives for the addition operation:

$$
f(x,y) = x + y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = 1 \hspace{0.5in} \frac{\partial f}{\partial y} = 1
$$

that is, the derivative on both \\(x,y\\) is one regardless of what the values of \\(x,y\\) are. This makes sense, since increasing either \\(x,y\\) would increase the output of \\(f\\), and the rate of that increase would be independent of what the actual values of \\(x,y\\) are (unlike the case of multiplication above). The last function we'll use quite a bit in the class is the *max* operation:

$$
f(x,y) = \max(x, y) \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = \mathbb{1}(x >= y) \hspace{0.5in} \frac{\partial f}{\partial y} = \mathbb{1}(y >= x)
$$

That is, the (sub)gradient is 1 on the input that was larger and 0 on the other input. Intuitively, if the inputs are \\(x = 4,y = 2\\), then the max is 4, and the function is not sensitive to the setting of \\(y\\). That is, if we were to increase it by a tiny amount \\(h\\), the function would keep outputting 4, and therefore the gradient is zero: there is no effect. Of course, if we were to change \\(y\\) by a large amount (e.g. larger than 2), then the value of \\(f\\) would change, but the derivatives tell us nothing about the effect of such large changes on the inputs of a function; They are only informative for tiny, infinitesimally small changes on the inputs, as indicated by the \\(\lim\_{h \rightarrow 0}\\) in its definition. -->

<span style="float: right">
	[Table of Contents](#top)
</span>

<br>
<br>
<br>

<a name='discuss'></a>