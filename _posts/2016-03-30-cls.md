---
layout: post
comments: false
title:  "Scene Classification 101"
excerpt: "We implement a classification system to solve the classical problem in computer vision, a final project that is accompanied with the vision course. Old, antique, out of date features are used to salute to classics in old times before the renaissance of deep learning."
date:   2016-03-30 12:00:00
mathjax: true
---

In this project we are required to implement a system to classify five scenes. We use classic methods with feature extraction of the images in a multi-scale framework (spatial pyramid matching) and then train a SVM classifier. The basic steps include:

- Feature Extraction: dense sampling of SIFT descriptors and many other features (HOG, LBP, GIST, etc). In this project, I use FREAK, SURF, BRISK plus SIFT features.

- Codebook Generation: k-means clustering on subset or all of the feature descriptors to learn the codebook. A reasonable codebook size is about 1,024.

- Image Representation: compute a histogram vector to represent each image in terms of distance frequency with the codebook. The basic method is called __descriptor quantization__. An important alternative is locality-constrained linear coding (LLC), whichs boosts performance.

- Spatial Pyramid Matching [(paper)](http://www-cvr.ai.uiuc.edu/ponce_grp/publication/paper/cvpr06b.pdf): 
Only local feature description does not have the spatial order of local descriptors, which limits the descriptive power in image representation. Instead we implement a spatial pyramid matching method to consider feature descriptors in different scales (levels). The following figure shows the gist.

<div class="imgcap">
<img src="/assets/cls/spm.png" width="500">
<div class="thecap" style="text-align:justify">Consider a 3-level spaticial pyramid matching. Each level is devided into different grids and we obtain the corresponding image representations within each grid. Then we pool (max, mean or absolute, etc.) these results within each level and concatenate the pooled representations among levels with weights. There are three types of features in this particular example.</div>
</div>

- Classifier Learning: one is linear SVM and its alternative is kernel SVM.


* Finally, the [Source Code]() (written in C++ and tested on Visual Studio 2010) is available. 

#### The Baseline Model

The basic pipeline is `sift -> k-means codebook -> descriptor quntization -> linear SVM`, and achieves around 32% accuracy.


### Tuning Parameters and Adding Upgraded Components

In general, there are some rules of thumb (aka, failure experiences):

- Linear vs kernel SVM: I tried the radius kernel, which is the default setting in OpenCV and it sucks. Maybe it's related to the data patterns (sparse or not, for example) of the descriptors. So linear form will suffice.

- Feature selection: it is of vital importance. Good features are everything. I think this is the main reason why I did not achieve a satisfying result in essence.

- Memory bottleneck: the most surprising part during degugging is that the computer 
(16G ROM) keeps telling `insufficient mem alloc in opencv` although I checked the system monitor that the memory didn't be fully occupied at all! So weird. Details: after feature extraction, we have, say `30w x feat_dim` descriptor matrix to compute the codebook using k-means (choosing 1024 from 30w!), the matrix is so large and thus the program crashes. Solution is to reduce the number of descriptors in dense sampling (parameter `max_kps`).

- Misc: some minor experiments: codebook size (1024 vs 1500), the accuracy won't alter much. pooling method: max is best and others perform way much worse. Coefficients in concatenating representations in SPM: (0.25, 0.25, 0.5) is best. 





### Final Model and Result Analysis

### Acknowledgement




